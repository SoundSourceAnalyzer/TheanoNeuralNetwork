{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the imports are not useful yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from yaafelib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download GTZAN dataset once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadGTZAN():\n",
    "    filename = 'genres.tar.gz'\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve('http://opihi.cs.uvic.ca/sound/genres.tar.gz', filename)\n",
    "    else:\n",
    "        print('File ' + filename + ' exists')\n",
    "    \n",
    "    return filename\n",
    "filename = downloadGTZAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract files from .tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting genres.tar.gz \n",
      "['genres/blues', 'genres/classical', 'genres/country', 'genres/disco', 'genres/hiphop', 'genres/jazz', 'genres/metal', 'genres/pop', 'genres/reggae', 'genres/rock']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "def extract(filename):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root):\n",
    "        print('File %s already extracted' % (root))\n",
    "    else:\n",
    "        print('Extracting %s ' % filename)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "                    if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('Expected %d folders not found.' % (num_classes))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "folders = extract(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(audio_file):\n",
    "    if os.path.exists(audio_file):\n",
    "        print('Getting features from ' + audio_file)\n",
    "    else:\n",
    "        raise Exception('File ' + audio_file + ' not found')\n",
    "    fp = FeaturePlan(sample_rate=22050, normalize=1)\n",
    "    # Features that seems to be most often used, so they are good to start with.\n",
    "    fp.addFeature('mfcc: MFCC')\n",
    "    fp.addFeature('zcr: ZCR')\n",
    "    fp.addFeature('spectral_shape: SpectralShapeStatistics')\n",
    "    fp.addFeature('magnitude_spectrum: MagnitudeSpectrum')\n",
    "    df = fp.getDataFlow()\n",
    "    engine = Engine()\n",
    "    engine.load(df)\n",
    "    afp = AudioFileProcessor()\n",
    "    afp.setOutputFormat('csv', 'features', {'Precision': '8', 'Metadata': 'False'})\n",
    "    afp.processFile(engine, audio_file)\n",
    "    engine.flush()\n",
    "    feats = engine.readAllOutputs()\n",
    "    return feats\n",
    "\n",
    "def load_genre(folder):\n",
    "    print('Loading genre from folder ' + folder)\n",
    "    samples = os.listdir(folder)\n",
    "    dataset =[]\n",
    "    for sample in os.listdir(folder):\n",
    "        sample_file = os.path.join(folder, sample)\n",
    "        if sample.endswith('.au'):\n",
    "            features = get_features(sample_file)\n",
    "            dataset.append(features)\n",
    "    return dataset\n",
    "\n",
    "def pickle(data_folders):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename):\n",
    "            print('%s already pickled' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_genre(folder)\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return dataset_names\n",
    "\n",
    "pickled_datasets = pickle(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is pickled, thus the feature extraction doesn't have to be repeated. \n",
    "\n",
    "Let's vizualize something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "def visualize(filename, sample, feature):\n",
    "    with open(filename, 'r') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "        f = unpickled[sample][feature]\n",
    "        plt.plot(f[0])\n",
    "        plt.ylabel(feature)\n",
    "        plt.show()\n",
    "\n",
    "visualize('genres/rock.pickle', 0, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 1, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 2, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 3, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 0, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 1, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 2, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 3, 'magnitude_spectrum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set and test set are needed. In addition, they have to be shuffeled/randomized somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickled_datasets = ['blues.pickle','classical.pickle','country.pickle','disco.pickle','hiphop.pickle','jazz.pickle',\n",
    "                    'metal.pickle','pop.pickle','reggae.pickle','rock.pickle']\n",
    "\n",
    "def unpickle(filename):\n",
    "    with open('genres/' + filename, 'r') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def evens(dataset):\n",
    "    return dataset[::2]\n",
    "def odds(dataset):\n",
    "    return dataset[1::2]\n",
    "\n",
    "def dataset_with_labels(filename):\n",
    "    dataset = unpickle(filename)\n",
    "    return dataset, [os.path.splitext(filename)[0] for count in xrange(len(dataset))]\n",
    "\n",
    "datasets = map(dataset_with_labels, pickled_datasets)\n",
    "features = [features for genre in datasets for features in genre[0]]\n",
    "labels = [label for genre in datasets for label in genre[1]]\n",
    "\n",
    "train_dataset = odds(features)\n",
    "train_labels = odds(labels)\n",
    "test_dataset = evens(features)\n",
    "test_labels = evens(labels)\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(len(labels))\n",
    "    shuffled_dataset = np.asarray(dataset)[permutation]\n",
    "    shuffled_labels = np.asarray(labels)[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "\n",
    "try:\n",
    "    f = open('train_and_test_data.pickle', 'wb')\n",
    "    save = {\n",
    "        'train_features': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'test_features': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can go to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start here\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train_and_test_data.pickle', 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_features']\n",
    "    train_labels = save['train_labels']\n",
    "    test_dataset = save['test_features']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # gc\n",
    "train_dataset = ([d['magnitude_spectrum'][0] for d in train_dataset])\n",
    "test_dataset = ([d['magnitude_spectrum'][0] for d in test_dataset])\n",
    "magnitude = 513\n",
    "num_labels = 10 # genres\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def oneHotEncoder(pos, max):\n",
    "    encoded = []\n",
    "    for i in range(0, max):\n",
    "        if i == pos:\n",
    "            encoded.append(1)\n",
    "        else:\n",
    "            encoded.append(0)\n",
    "    return encoded\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = np.asarray(dataset).reshape((-1, magnitude, 1, 1)).astype(np.float32)\n",
    "    genres = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']\n",
    "    labels = map(lambda x: np.int32(genres.index(x)), labels)\n",
    "    labels = map(lambda x: oneHotEncoder(x, num_labels), labels)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.25, 0.5, 0.75, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self, n_features):\n",
    "        hidden_layer_size = 5\n",
    "        l2_regularisation = 0.001\n",
    "\n",
    "        input_vector = T.fvector('input_vector')\n",
    "        target_value = T.fscalar('target_value')\n",
    "        learningrate = T.fscalar('learningrate')\n",
    "\n",
    "        W_hidden_vals = np.asarray(rng.normal(loc=0.0,\n",
    "                                                 scale=0.1,\n",
    "                                                 size=(n_features, hidden_layer_size)),\n",
    "                                      dtype=floatX)\n",
    "        W_hidden = theano.shared(W_hidden_vals, 'W_hidden')\n",
    "\n",
    "        hidden = T.dot(input_vector, W_hidden)\n",
    "        hidden = T.nnet.sigmoid(hidden)\n",
    "\n",
    "        W_output_vals = np.asarray(rng.normal(loc=0.0,\n",
    "                                                 scale=0.1,\n",
    "                                                 size=(hidden_layer_size, 1)),\n",
    "                                      dtype=floatX)\n",
    "        W_output = theano.shared(W_output_vals, 'W_output')\n",
    "\n",
    "        predicted_value = T.dot(hidden, W_output)\n",
    "        predicted_value = T.nnet.sigmoid(predicted_value)\n",
    "\n",
    "        cost = T.sqr(predicted_value - target_value).sum()\n",
    "        cost += l2_regularisation * (T.sqr(W_hidden).sum() + T.sqr(W_output).sum())\n",
    "\n",
    "        params = [W_hidden, W_output]\n",
    "        gradients = T.grad(cost, params)\n",
    "        updates = [(p, p - (learningrate * g)) for p, g in zip(params, gradients)]\n",
    "\n",
    "        self.train = theano.function(inputs=[input_vector, target_value, learningrate],\n",
    "                                     outputs=[cost, predicted_value],\n",
    "                                     updates=updates,\n",
    "                                     allow_input_downcast=True)\n",
    "\n",
    "        self.test = theano.function(inputs=[input_vector, target_value],\n",
    "                                    outputs=[cost, predicted_value],\n",
    "                                    allow_input_downcast=True)\n",
    "\n",
    "\n",
    "def create_random_dataset(num_of_classes, num_of_vectors, num_of_features, feature_min_value, features_max_value):\n",
    "    dataset = []\n",
    "    for i in range(num_of_vectors):\n",
    "        classify_as = rng.randint(1, num_of_classes)\n",
    "        features_vector = rng.uniform(feature_min_value, features_max_value, num_of_features)\n",
    "        dataset.append((classify_as, features_vector))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_classes_ranges(num_of_classes, min_value, max_value):\n",
    "    weights_ranges = [min_value]\n",
    "    diff = (max_value - min_value) / num_of_classes\n",
    "    previous = min_value\n",
    "    for i in range(1, num_of_classes):\n",
    "        previous += diff\n",
    "        weights_ranges.append(previous)\n",
    "    weights_ranges.append(max_value)\n",
    "    return weights_ranges\n",
    "\n",
    "\n",
    "learningrate = 0.1\n",
    "epochs = 25\n",
    "\n",
    "num_classes = 4\n",
    "num_features = 4\n",
    "data_train_size = 100\n",
    "data_test_size = 20\n",
    "\n",
    "data_train = create_random_dataset(num_classes, data_train_size, num_features, -1.0, 1.0)\n",
    "data_test = create_random_dataset(num_classes, data_test_size, num_features, -1.0, 1.0)\n",
    "\n",
    "weights_values = create_classes_ranges(num_classes, 0.0, 1.0)\n",
    "print(weights_values)\n",
    "\n",
    "classifier = Classifier(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge field for improvements here:\n",
    "* Starting points should not be randomized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Training & Testing:\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network test...\n",
      "Number of classes:  4\n",
      "Number of features:  4\n",
      "Epochs:  25\n",
      "Training data:\n",
      "[(1, array([ 0.18859198,  0.83224269,  0.23613818, -0.91101835])), (2, array([-0.22595652, -0.32682014, -0.63354295,  0.04799655])), (1, array([-0.40658881,  0.05777961, -0.57866686, -0.93151891])), (3, array([ 0.54505641, -0.95898673, -0.30147757,  0.38201886])), (3, array([ 0.99996535,  0.28485004,  0.26605391,  0.72069321])), (1, array([-0.95753242, -0.0036907 ,  0.72367905,  0.02785985])), (1, array([-0.93181203, -0.85121938,  0.06947845, -0.38879619])), (3, array([ 0.80214545,  0.25432567, -0.15890534,  0.16948516])), (3, array([-0.8441584 , -0.55379886, -0.27441706, -0.56673955])), (2, array([-0.40939481,  0.08088165, -0.87541221, -0.02193611])), (2, array([ 0.87229882,  0.0606048 , -0.55171368, -0.87291553])), (3, array([-0.53220096,  0.62423546, -0.14385187, -0.32438767])), (1, array([ 0.99243767,  0.99014379,  0.57228016,  0.69527707])), (3, array([ 0.81119975, -0.87311573, -0.30597218,  0.98207087])), (3, array([ 0.41769888, -0.71645577,  0.44323169,  0.92429187])), (3, array([ 0.33131554, -0.53610591,  0.85011976, -0.9007742 ])), (2, array([ 0.01163206,  0.43545413,  0.50447913, -0.44177669])), (2, array([-0.34260325, -0.59315487, -0.5116947 ,  0.29587348])), (2, array([-0.38508111,  0.536606  , -0.20486461,  0.14217628])), (1, array([ 0.37886844,  0.21565768,  0.71932167, -0.63574557])), (1, array([ 0.66414127,  0.98353471, -0.24706839, -0.06532525])), (2, array([-0.87485255, -0.20685018, -0.26611927, -0.24628165])), (1, array([-0.15771198, -0.32410192, -0.09040134, -0.4301893 ])), (3, array([-0.45677617,  0.55968786,  0.99447973, -0.25823112])), (3, array([ 0.346878  ,  0.39857741, -0.18014045,  0.33852582])), (1, array([-0.08181672, -0.27532721,  0.5976631 ,  0.44328085])), (1, array([ 0.75926842,  0.15744956, -0.56801492,  0.31364181])), (1, array([ 0.92945881,  0.62433963, -0.16992409, -0.32616526])), (3, array([-0.68544383,  0.00498315,  0.81751998,  0.78841101])), (3, array([ 0.06409715,  0.33471639,  0.58855876, -0.74564481])), (2, array([-0.24091639,  0.39131024,  0.11698871, -0.96712267])), (1, array([-0.06153675, -0.75048908,  0.15320933, -0.96624859])), (2, array([-0.00788246,  0.2398243 ,  0.10540365,  0.99270862])), (3, array([-0.8823038 ,  0.95463326, -0.54243479, -0.86862286])), (2, array([-0.76043384,  0.80702554, -0.49775821,  0.62831827])), (1, array([ 0.67765082, -0.08726535,  0.28873543, -0.42411051])), (3, array([ 0.11462055,  0.49805156,  0.51237388, -0.8734435 ])), (2, array([ 0.89519153, -0.36751801,  0.64743232, -0.3410244 ])), (2, array([-0.92940936, -0.61762256,  0.23355973, -0.54811872])), (2, array([ 0.0854856 , -0.0651094 , -0.08862225,  0.1948723 ])), (1, array([ 0.30109893,  0.69767674,  0.09634672,  0.40334145])), (3, array([-0.06507885,  0.97904293,  0.01277652, -0.15156444])), (1, array([-0.30274206, -0.01431301,  0.18470966,  0.07998236])), (2, array([ 0.9708535 ,  0.23448   ,  0.51032225, -0.44617177])), (2, array([ 0.39208367, -0.15864559,  0.94133233,  0.78025442])), (2, array([ 0.72246468,  0.17890142, -0.27245111,  0.88613143])), (2, array([-0.02551195, -0.08992648, -0.49399625, -0.45406202])), (1, array([ 0.22030201,  0.93418933, -0.24063716, -0.73220531])), (2, array([ 0.72311387,  0.36683644,  0.97371884, -0.53179592])), (1, array([-0.92732872, -0.59453796, -0.71280308,  0.14732802])), (1, array([-0.26114784,  0.95679671, -0.70723495, -0.46425347])), (1, array([-0.13701602,  0.25076804, -0.46893243,  0.06669375])), (1, array([-0.29960895,  0.56668059,  0.66195311, -0.95372723])), (3, array([-0.32443925, -0.94265407,  0.39355917,  0.55608738])), (2, array([ 0.06403252, -0.06691052,  0.35898746,  0.10541876])), (1, array([ 0.14959514,  0.72491527,  0.44125886,  0.21824313])), (1, array([ 0.40653404,  0.04641415,  0.26165898,  0.33110729])), (1, array([-0.93487042, -0.4480793 , -0.72337674, -0.94746221])), (1, array([-0.25231231,  0.95617727,  0.90717183,  0.74447306])), (1, array([ 0.03526181, -0.04328575,  0.79225052,  0.56365227])), (2, array([-0.65263067, -0.33207786, -0.12395656,  0.02714914])), (1, array([ 0.10689882, -0.69117627,  0.40039176,  0.01078756])), (2, array([-0.04251479, -0.71438915, -0.26718788,  0.86132721])), (2, array([ 0.95921408,  0.88919608, -0.50340361,  0.06983414])), (3, array([ 0.60826741, -0.75253033,  0.08061886, -0.3529733 ])), (2, array([ 0.55774721, -0.61757468, -0.62478295, -0.15778549])), (2, array([ 0.01351514, -0.98351932, -0.49252119, -0.68352873])), (3, array([-0.34999427, -0.72873249, -0.48217598,  0.10768768])), (3, array([-0.79976949, -0.94286555, -0.99205494,  0.51268748])), (2, array([-0.96522802, -0.00737204,  0.46384291,  0.65476362])), (2, array([ 0.33269959, -0.1824147 ,  0.61874761, -0.34093655])), (1, array([ 0.52685695, -0.68117845, -0.47065662, -0.45681639])), (2, array([-0.53261152, -0.42793137,  0.22726996, -0.52680379])), (3, array([ 0.80114572, -0.58044788,  0.64972788,  0.02730014])), (2, array([ 0.28630852, -0.16719796,  0.77159304,  0.15571936])), (2, array([-0.57205386, -0.27386142, -0.58830555,  0.67580084])), (2, array([-0.82916796, -0.6879708 , -0.76348199,  0.82598972])), (2, array([-0.68810684,  0.68095249,  0.07313493,  0.36745183])), (1, array([ 0.40539423, -0.04558108,  0.57291561,  0.89901717])), (2, array([ 0.31761485,  0.51131445,  0.80994954,  0.72119016])), (3, array([-0.32924981, -0.92653766,  0.37175428,  0.30894659])), (3, array([ 0.7715518 , -0.96546238,  0.44593732, -0.58188327])), (2, array([ 0.23522917,  0.5403742 , -0.83074744, -0.72673904])), (2, array([ 0.33104936, -0.97978954, -0.86753567, -0.40695404])), (1, array([ 0.09767212,  0.37443595,  0.01659454, -0.54040503])), (2, array([-0.27921526, -0.30590128, -0.45040835, -0.5320397 ])), (2, array([-0.28369317, -0.50787496,  0.52715445,  0.21180047])), (2, array([-0.94565079, -0.06941219,  0.87053244, -0.34623607])), (2, array([ 0.0918201 , -0.40404503, -0.72896894,  0.92780443])), (3, array([ 0.62533008,  0.38821922,  0.42701841, -0.64127812])), (1, array([ 0.97493836, -0.33179971, -0.39435797,  0.88706337])), (1, array([-0.7950146 ,  0.98539061, -0.71875634,  0.77819821])), (3, array([-0.76775125, -0.79469127, -0.9091386 , -0.83560797])), (3, array([-0.60799711, -0.36817772, -0.17760274,  0.62442179])), (2, array([-0.23131754, -0.2770116 , -0.09079021, -0.70462043])), (1, array([ 0.15226728,  0.00658064, -0.81955559,  0.32656071])), (3, array([-0.35145458, -0.23432688, -0.44757055, -0.32177464])), (1, array([-0.83562738,  0.69963946, -0.17754954, -0.80895711])), (1, array([ 0.21955628, -0.234074  , -0.03106436,  0.05096075])), (1, array([ 0.20054224,  0.8549509 ,  0.16609751,  0.59823063]))]\n",
      "Testing data:\n",
      "[(1, array([ 0.03766234, -0.51684338,  0.66665628, -0.00805633])), (1, array([-0.91918092,  0.86182735, -0.7124126 ,  0.06617727])), (3, array([-0.21242794, -0.62673016,  0.69395119,  0.8237167 ])), (1, array([-0.10562214, -0.05699813,  0.05948721,  0.09999669])), (1, array([ 0.06990034, -0.93033048,  0.97760597, -0.11440007])), (3, array([-0.64345031,  0.0464214 , -0.68738628,  0.29271366])), (1, array([-0.75307592, -0.86826209,  0.20153855,  0.86217167])), (1, array([-0.29955282, -0.48743778, -0.40642733, -0.87787528])), (3, array([-0.25427278, -0.43575241, -0.26035708,  0.41437596])), (2, array([-0.96685082, -0.45269592,  0.06807312, -0.77984656])), (1, array([-0.15693178,  0.10449687, -0.35682108, -0.61286183])), (2, array([ 0.3180175 , -0.68504241, -0.12239955, -0.41229774])), (1, array([-0.91318256,  0.75361755, -0.5530915 ,  0.33628844])), (1, array([ 0.88867954, -0.3698882 ,  0.37245433,  0.37426579])), (2, array([-0.31487146, -0.80028854, -0.96177507, -0.41408706])), (3, array([ 0.73992531, -0.93980238, -0.84837089,  0.02436046])), (1, array([ 0.41922481,  0.97288318,  0.93083023,  0.53570722])), (1, array([-0.41566529,  0.48244523, -0.91805519,  0.8862954 ])), (2, array([ 0.21975114,  0.21841232,  0.42685157,  0.8214695 ])), (3, array([ 0.45656834, -0.16879739,  0.04494262, -0.27457743]))]\n",
      "Epoch: 0 Cost: 171.441708725 , Accuracy: 0.24\n",
      "Epoch: 1 Cost: 150.04814257 , Accuracy: 0.26\n",
      "Epoch: 2 Cost: 147.87325077 , Accuracy: 0.26\n",
      "Epoch: 3 Cost: 147.100018695 , Accuracy: 0.26\n",
      "Epoch: 4 Cost: 146.729739806 , Accuracy: 0.26\n",
      "Epoch: 5 Cost: 146.524940554 , Accuracy: 0.26\n",
      "Epoch: 6 Cost: 146.401504126 , Accuracy: 0.26\n",
      "Epoch: 7 Cost: 146.322729542 , Accuracy: 0.26\n",
      "Epoch: 8 Cost: 146.270362784 , Accuracy: 0.26\n",
      "Epoch: 9 Cost: 146.234466491 , Accuracy: 0.26\n",
      "Epoch: 10 Cost: 146.209261835 , Accuracy: 0.26\n",
      "Epoch: 11 Cost: 146.191215552 , Accuracy: 0.26\n",
      "Epoch: 12 Cost: 146.178081038 , Accuracy: 0.26\n",
      "Epoch: 13 Cost: 146.1683844 , Accuracy: 0.26\n",
      "Epoch: 14 Cost: 146.161133787 , Accuracy: 0.26\n",
      "Epoch: 15 Cost: 146.155647609 , Accuracy: 0.26\n",
      "Epoch: 16 Cost: 146.151449239 , Accuracy: 0.26\n",
      "Epoch: 17 Cost: 146.148200443 , Accuracy: 0.26\n",
      "Epoch: 18 Cost: 146.14565818 , Accuracy: 0.26\n",
      "Epoch: 19 Cost: 146.143645933 , Accuracy: 0.26\n",
      "Epoch: 20 Cost: 146.142034307 , Accuracy: 0.26\n",
      "Epoch: 21 Cost: 146.140727684 , Accuracy: 0.26\n",
      "Epoch: 22 Cost: 146.139654899 , Accuracy: 0.26\n",
      "Epoch: 23 Cost: 146.138762649 , Accuracy: 0.26\n",
      "Epoch: 24 Cost: 146.138010784 , Accuracy: 0.26\n",
      "Test_cost: 0.0224451335797, Test_accuracy: 0.0\n",
      "Test_cost: 0.0449237669935, Test_accuracy: 0.0\n",
      "Test_cost: 4.08357244684, Test_accuracy: 0.333333333333\n",
      "Test_cost: 4.10602639586, Test_accuracy: 0.25\n",
      "Test_cost: 4.1284666768, Test_accuracy: 0.2\n",
      "Test_cost: 8.17275445908, Test_accuracy: 0.333333333333\n",
      "Test_cost: 8.19519798031, Test_accuracy: 0.285714285714\n",
      "Test_cost: 8.21764376621, Test_accuracy: 0.25\n",
      "Test_cost: 12.2583669283, Test_accuracy: 0.333333333333\n",
      "Test_cost: 13.2884793461, Test_accuracy: 0.3\n",
      "Test_cost: 13.3109350611, Test_accuracy: 0.272727272727\n",
      "Test_cost: 14.3413547086, Test_accuracy: 0.25\n",
      "Test_cost: 14.3638304366, Test_accuracy: 0.230769230769\n",
      "Test_cost: 14.3862815641, Test_accuracy: 0.214285714286\n",
      "Test_cost: 15.4167141007, Test_accuracy: 0.2\n",
      "Test_cost: 19.4557316059, Test_accuracy: 0.25\n",
      "Test_cost: 19.4782103607, Test_accuracy: 0.235294117647\n",
      "Test_cost: 19.5006872713, Test_accuracy: 0.222222222222\n",
      "Test_cost: 20.534611631, Test_accuracy: 0.210526315789\n",
      "Test_cost: 24.5762622614, Test_accuracy: 0.25\n",
      "Number of correct guesses:  5  out of  20\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural network test...\")\n",
    "print(\"Number of classes: \", num_classes)\n",
    "print(\"Number of features: \", num_features)\n",
    "print(\"Epochs: \", epochs)\n",
    "print(\"Training data:\")\n",
    "print(data_train)\n",
    "print(\"Testing data:\")\n",
    "print(data_test)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cost_sum = 0.0\n",
    "    correct = 0\n",
    "    for label, vector in data_train:\n",
    "        cost, predicted_value = classifier.train(vector, label, learningrate)\n",
    "        cost_sum += cost\n",
    "        range_index = int(label)\n",
    "        if (predicted_value >= weights_values[range_index]) and (predicted_value < weights_values[range_index + 1]):\n",
    "            correct += 1\n",
    "    print(\"Epoch: \" + str(epoch) + \" Cost: \" + str(cost_sum), \", Accuracy: \" + str(float(correct) / len(data_train)))\n",
    "\n",
    "cost_sum = 0.0\n",
    "correct = 0\n",
    "index = 0\n",
    "for label, vector in data_test:\n",
    "    index += 1\n",
    "    cost, predicted_value = classifier.test(vector, label)\n",
    "    cost_sum += cost\n",
    "    range_index = int(label)\n",
    "    if (predicted_value >= weights_values[range_index]) and (predicted_value < weights_values[range_index + 1]):\n",
    "        correct += 1\n",
    "    print(\"Test_cost: \" + str(cost_sum) + \", Test_accuracy: \" + str(float(correct) / index))\n",
    "print(\"Number of correct guesses: \", correct, \" out of \", data_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Classifier is disastrous. Has to be enhanced, but it's somehow working..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
